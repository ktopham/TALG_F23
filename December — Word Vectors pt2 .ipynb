{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "## As used in [\"Word embeddings quantify 100 years of gender and ethnic stereotypes\"](https://doi.org/10.1073/pnas.1720347115)\n",
    "\n",
    "The article by Garg et al. investigates and validates the use of machine-learned word embeddings to study biases in language:\n",
    "\n",
    "```\n",
    "\"In word-embedding models, each word in a given language is assigned to a high-dimensional vector such that the geometry of the vectors captures semantic relations between the words — e.g., vectors being closer together has been shown to correspond to more similar words.\"\n",
    "```\n",
    "\n",
    "Using pre-trained models of large text corpora, the authors evaluate vectors of words relating to gender and ethnicity against \"neutral\" word categories to measure bias.\n",
    "\n",
    "### Load model(s) and look at vectors\n",
    "\n",
    "```\n",
    "\"For contemporary snapshot analysis, we use the standard Google News word2vec vectors trained on the Google News dataset.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of each word's vector (for this model):\", len(model[\"cat\"]))\n",
    "print(\"Sample vector for 'cat':\")\n",
    "print(model[\"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build a model of your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "new_model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# new_model.save(\"word2vec.model\")\n",
    "new_model.wv[\"computer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "with open(\"KafkaMetamorphosis.txt\") as f:\n",
    "    text = f.read().lower()\n",
    "    sentences_1 = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "\n",
    "kafka_model = Word2Vec(sentences=sentences_1, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# new_model.save(\"word2vec.model\")\n",
    "kafka_model.wv.most_similar(\"vermin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you do with word vectors?\n",
    "\n",
    "The [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) (API) lists out all the associated functions and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similar_by_word(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"democracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"man\" is to \"king\" as \"woman\" is to _______\n",
    "\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"paltry\" is to \"significance\" as \"banal\" is to _______\n",
    "\n",
    "model.most_similar(positive=['banal', 'significance'], negative=['paltry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"clumsy\" is to \"botch\" as \"lazy\" is to ________\n",
    "\n",
    "model.most_similar(positive=['botch', 'lazy'], negative=['clumsy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"freedom\", topn=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Average Embeddings\n",
    "\n",
    "```\n",
    "\"We first compute the average embedding distance between words that represent women—e.g., she, female—and words for occupations—e.g., teacher, lawyer.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of vectors among all words of each category.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "woman_words = [\"she\", \"daughter\", \"hers\", \"her\", \"mother\", \"woman\", \"girl\", \"herself\", \"female\", \"sister\", \"daughters\", \"mothers\", \"women\",\n",
    "\"girls\", \"femen\", \"sisters\", \"aunt\", \"aunts\", \"niece\", \"nieces\"]\n",
    "\n",
    "man_words = [\"he\", \"son\", \"his\", \"him\", \"father\", \"man\", \"boy\", \"himself\", \"male\", \"brother\", \"sons\", \"fathers\", \"men\", \"boys\", \"males\", \"brothers\", \"uncle\",\n",
    "\"uncles\", \"nephew\", \"nephews\"]\n",
    "\n",
    "occupations = [\"janitor\", \"statistician\", \"midwife\", \"bailiff\", \"auctioneer\", \"photographer\", \"geologist\", \"shoemaker\", \"athlete\", \"cashier\",\n",
    "\"dancer\", \"housekeeper\", \"accountant\", \"physicist\", \"gardener\", \"dentist\", \"weaver\", \"blacksmith\", \"psychologist\", \"supervisor\",\n",
    "\"mathematician\", \"surveyor\", \"tailor\", \"designer\", \"economist\", \"mechanic\", \"laborer\", \"postmaster\", \"broker\", \"chemist\", \"librarian\", \"attendant\", \"clerical\", \"musician\", \"porter\", \"scientist\", \"carpenter\", \"sailor\", \"instructor\", \"sheriff\", \"pilot\", \"inspector\", \"mason\",\n",
    "\"baker\", \"administrator\", \"architect\", \"collector\", \"operator\", \"surgeon\", \"driver\", \"painter\", \"conductor\", \"nurse\", \"cook\", \"engineer\",\n",
    "\"retired\", \"sales\", \"lawyer\", \"clergy\", \"physician\", \"farmer\", \"clerk\", \"manager\", \"guard\", \"artist\", \"smith\", \"official\", \"police\", \"doctor\",\n",
    "\"professor\", \"student\", \"judge\", \"teacher\", \"author\", \"secretary\", \"soldier\"]\n",
    "\n",
    "prof_occupations = [\"statistician\", \"auctioneer\", \"photographer\", \"geologist\", \"accountant\", \"physicist\", \"dentist\", \"psychologist\", \"supervisor\", \"mathematician\", \"designer\", \"economist\", \"postmaster\", \"broker\", \"chemist\", \"librarian\", \"scientist\", \"instructor\",\n",
    "\"pilot\", \"administrator\", \"architect\", \"surgeon\", \"nurse\", \"engineer\", \"lawyer\", \"physician\", \"manager\", \"official\", \"doctor\", \"professor\",\n",
    "\"student\", \"judge\", \"teacher\", \"author\"]\n",
    "\n",
    "personality_traits = ['disorganized', 'devious', 'impressionable', 'circumspect', 'impassive', 'aimless', 'effeminate', 'unfathomable', 'fickle', 'unprincipled', 'inoffensive', 'reactive', 'providential', 'resentful', 'bizarre', 'impractical', 'sarcastic', 'misguided', 'imitative', 'pedantic', 'venomous', 'erratic', 'insecure', 'resourceful', 'neurotic', 'forgiving', 'profligate', 'whimsical', 'assertive', 'incorruptible', 'individualistic', 'faithless', 'disconcerting', 'barbaric', 'hypnotic', 'vindictive', 'observant', 'dissolute', 'frightening', 'complacent', 'boisterous', 'pretentious', 'disobedient', 'tasteless', 'sedentary', 'sophisticated', 'regimental', 'mellow', 'deceitful', 'impulsive', 'playful', 'sociable', 'methodical', 'willful', 'idealistic', 'boyish', 'callous', 'pompous', 'unchanging', 'crafty', 'punctual', 'compassionate', 'intolerant', 'challenging', 'scornful', 'possessive', 'conceited', 'imprudent', 'dutiful', 'lovable', 'disloyal', 'dreamy', 'appreciative', 'forgetful', 'unrestrained', 'forceful', 'submissive', 'predatory', 'fanatical', 'illogical', 'tidy', 'aspiring', 'studious', 'adaptable', 'conciliatory', 'artful', 'thoughtless', 'deceptive', 'frugal', 'reflective', 'insulting', 'unreliable', 'stoic', 'hysterical', 'rustic', 'inhibited', 'outspoken', 'unhealthy', 'ascetic', 'skeptical', 'painstaking', 'contemplative', 'leisurely', 'sly', 'mannered', 'outrageous', 'lyrical', 'placid', 'cynical', 'irresponsible', 'vulnerable', 'arrogant', 'persuasive', 'perverse', 'steadfast', 'crisp', 'envious', 'naive', 'greedy', 'presumptuous', 'obnoxious', 'irritable', 'dishonest', 'discreet', 'sporting', 'hateful', 'ungrateful', 'frivolous', 'reactionary', 'skillful', 'cowardly', 'sordid', 'adventurous', 'dogmatic', 'intuitive', 'bland', 'indulgent', 'discontented', 'dominating', 'articulate', 'fanciful', 'discouraging', 'treacherous', 'repressed', 'moody', 'sensual', 'unfriendly', 'optimistic', 'clumsy', 'contemptible', 'focused', 'haughty', 'morbid', 'disorderly', 'considerate', 'humorous', 'preoccupied', 'airy', 'impersonal', 'cultured', 'trusting', 'respectful', 'scrupulous', 'scholarly', 'superstitious', 'tolerant', 'realistic', 'malicious', 'irrational', 'sane', 'colorless', 'masculine', 'witty', 'inert', 'prejudiced', 'fraudulent', 'blunt', 'childish', 'brittle', 'disciplined', 'responsive', 'courageous', 'bewildered', 'courteous', 'stubborn', 'aloof', 'sentimental', 'athletic', 'extravagant', 'brutal', 'manly', 'cooperative', 'unstable', 'youthful', 'timid', 'amiable', 'retiring', 'fiery', 'confidential', 'relaxed', 'imaginative', 'mystical', 'shrewd', 'conscientious', 'monstrous', 'grim', 'questioning', 'lazy', 'dynamic', 'gloomy', 'troublesome', 'abrupt', 'eloquent', 'dignified', 'hearty', 'gallant', 'benevolent', 'maternal', 'paternal', 'patriotic', 'aggressive', 'competitive', 'elegant', 'flexible', 'gracious', 'energetic', 'tough', 'contradictory', 'shy', 'careless', 'cautious', 'polished', 'sage', 'tense', 'caring', 'suspicious', 'sober', 'neat', 'transparent', 'disturbing', 'passionate', 'obedient', 'crazy', 'restrained', 'fearful', 'daring', 'prudent', 'demanding', 'impatient', 'cerebral', 'calculating', 'amusing', 'honorable', 'casual', 'sharing', 'selfish', 'ruined', 'spontaneous', 'admirable', 'conventional', 'cheerful', 'solitary', 'upright', 'stiff', 'enthusiastic', 'petty', 'dirty', 'subjective', 'heroic', 'stupid', 'modest', 'impressive', 'orderly', 'ambitious', 'protective', 'silly', 'alert', 'destructive', 'exciting', 'crude', 'ridiculous', 'subtle', 'mature', 'creative', 'coarse', 'passive', 'oppressed', 'accessible', 'charming', 'clever', 'decent', 'miserable', 'superficial', 'shallow', 'stern', 'winning', 'balanced', 'emotional', 'rigid', 'invisible', 'desperate', 'cruel', 'romantic', 'agreeable', 'hurried', 'sympathetic', 'solemn', 'systematic', 'vague', 'peaceful', 'humble', 'dull', 'expedient', 'loyal', 'decisive', 'arbitrary', 'earnest', 'confident', 'conservative', 'foolish', 'moderate', 'helpful', 'delicate', 'gentle', 'dedicated', 'hostile', 'generous', 'reliable', 'dramatic', 'precise', 'calm', 'healthy', 'attractive', 'artificial', 'progressive', 'odd', 'confused', 'rational', 'brilliant', 'intense', 'genuine', 'mistaken', 'driving', 'stable', 'objective', 'sensitive', 'neutral', 'strict', 'angry', 'profound', 'smooth', 'ignorant', 'thorough', 'logical', 'intelligent', 'extraordinary', 'experimental', 'steady', 'formal', 'faithful', 'curious', 'reserved', 'honest', 'busy', 'educated', 'liberal', 'friendly', 'efficient', 'sweet', 'surprising', 'mechanical', 'clean', 'critical', 'criminal', 'soft', 'proud', 'quiet', 'weak', 'anxious', 'solid', 'complex', 'grand', 'warm', 'slow', 'false', 'extreme', 'narrow', 'dependent', 'wise', 'organized', 'pure', 'directed', 'dry', 'obvious', 'popular', 'capable', 'secure', 'active', 'independent', 'ordinary', 'fixed', 'practical', 'serious', 'fair', 'understanding', 'constant', 'cold', 'responsible', 'deep', 'religious', 'private', 'simple', 'physical', 'original', 'working', 'strong', 'modern', 'determined', 'open', 'political', 'difficult', 'knowledge', 'kind']\n",
    "\n",
    "average_woman_words = np.mean(np.array([model[word] for word in woman_words if word in model]), axis = 0)\n",
    "average_man_words = np.mean(np.array([model[word] for word in man_words if word in model]), axis = 0)\n",
    "average_occupations = np.mean(np.array([model[word] for word in occupations if word in model]), axis = 0)\n",
    "average_prof_occupations = np.mean(np.array([model[word] for word in prof_occupations if word in model]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(average_woman_words))\n",
    "print(len(average_man_words))\n",
    "print(len(average_occupations))\n",
    "print(len(average_prof_occupations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between occations and 'man' and 'woman' words.\n",
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c\n",
    "\n",
    "def calc_distance_between_vectors(vec1, vec2, distype = ''):\n",
    "    if distype == 'norm':\n",
    "        return np.linalg.norm(np.subtract(vec1, vec2))\n",
    "    else:\n",
    "        return cossim(vec1, vec2)\n",
    "\n",
    "occupations_to_woman = calc_distance_between_vectors(average_occupations, average_woman_words)\n",
    "occupations_to_man = calc_distance_between_vectors(average_occupations, average_man_words)\n",
    "print(\"Occupation distances (women, men):\", occupations_to_woman, occupations_to_man)\n",
    "\n",
    "prof_occupations_to_woman = calc_distance_between_vectors(average_prof_occupations, average_woman_words)\n",
    "prof_occupations_to_man = calc_distance_between_vectors(average_prof_occupations, average_man_words)\n",
    "print(\"Occupation distances (women, men):\", prof_occupations_to_woman, prof_occupations_to_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"A natural metric for the embedding bias is the average distance for women minus the average distance for men. If this value is negative, then the embedding more closely associates the occupations with men.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_bias = occupations_to_woman - occupations_to_man\n",
    "print(embedding_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output = []\n",
    "\n",
    "for occupation in occupations:\n",
    "    man = calc_distance_between_vectors(model[occupation], average_man_words)\n",
    "    woman = calc_distance_between_vectors(model[occupation], average_woman_words)\n",
    "    output.append([occupation, woman - man])  \n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(pd.DataFrame(output, columns=[\"Occupation\", \"Woman Bias\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for occupation in prof_occupations:\n",
    "    man = calc_distance_between_vectors(model[occupation], average_man_words)\n",
    "    woman = calc_distance_between_vectors(model[occupation], average_woman_words)\n",
    "    output.append([occupation, woman - man])  \n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(pd.DataFrame(output, columns=[\"Occupation\", \"Woman Bias\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for t in personality_traits:\n",
    "    man = calc_distance_between_vectors(model[t], average_man_words)\n",
    "    woman = calc_distance_between_vectors(model[t], average_woman_words)\n",
    "    output.append([t, woman - man])  \n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(pd.DataFrame(output, columns=[\"Trait\", \"Woman Bias\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Data\n",
    "\n",
    "Loading Sample COHA corpora, divided by decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1850-w.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a7c717a4430e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1850-w.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1850-w.npy'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "with open(\"1850-w.npy\", \"rb\") as f:\n",
    "    vectors = np.lib.format.read_array(f)\n",
    "\n",
    "with open(\"1850-vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "model1850s = KeyedVectors(vectors.shape[1])\n",
    "model1850s.add_vectors(vocab, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"1900-w.npy\", \"rb\") as f:\n",
    "    vectors = np.lib.format.read_array(f)\n",
    "\n",
    "with open(\"1900-vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "model1900s = KeyedVectors(vectors.shape[1])\n",
    "model1900s.add_vectors(vocab, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"1950-w.npy\", \"rb\") as f:\n",
    "    vectors = np.lib.format.read_array(f)\n",
    "\n",
    "with open(\"1950-vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "model1950s = KeyedVectors(vectors.shape[1])\n",
    "model1950s.add_vectors(vocab, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2000-w.npy\", \"rb\") as f:\n",
    "    vectors = np.lib.format.read_array(f)\n",
    "\n",
    "with open(\"2000-vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "model00s = KeyedVectors(vectors.shape[1])\n",
    "model00s.add_vectors(vocab, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data(fname):\n",
    "    with open(fname) as fin:\n",
    "        rlist = [n.replace(\"\\n\",\"\") for n in fin.readlines()]\n",
    "        return rlist\n",
    "\n",
    "white_names = compile_data(\"./data/names_white.txt\")\n",
    "\n",
    "asian_names = compile_data(\"./data/names_asian.txt\")\n",
    "\n",
    "black_names = compile_data(\"./data/names_black.txt\")\n",
    "\n",
    "words_christianity = compile_data(\"./data/words_christianity.txt\")\n",
    "\n",
    "words_islam = compile_data(\"./data/words_islam.txt\")\n",
    "\n",
    "adjectives_otherization = compile_data(\"./data/adjectives_otherization.txt\")\n",
    "\n",
    "\n",
    "personality_traits = ['disorganized', 'devious', 'impressionable', 'circumspect', 'impassive', 'aimless', 'effeminate', 'unfathomable', 'fickle', 'unprincipled', 'inoffensive', 'reactive', 'providential', 'resentful', 'bizarre', 'impractical', 'sarcastic', 'misguided', 'imitative', 'pedantic', 'venomous', 'erratic', 'insecure', 'resourceful', 'neurotic', 'forgiving', 'profligate', 'whimsical', 'assertive', 'incorruptible', 'individualistic', 'faithless', 'disconcerting', 'barbaric', 'hypnotic', 'vindictive', 'observant', 'dissolute', 'frightening', 'complacent', 'boisterous', 'pretentious', 'disobedient', 'tasteless', 'sedentary', 'sophisticated', 'regimental', 'mellow', 'deceitful', 'impulsive', 'playful', 'sociable', 'methodical', 'willful', 'idealistic', 'boyish', 'callous', 'pompous', 'unchanging', 'crafty', 'punctual', 'compassionate', 'intolerant', 'challenging', 'scornful', 'possessive', 'conceited', 'imprudent', 'dutiful', 'lovable', 'disloyal', 'dreamy', 'appreciative', 'forgetful', 'unrestrained', 'forceful', 'submissive', 'predatory', 'fanatical', 'illogical', 'tidy', 'aspiring', 'studious', 'adaptable', 'conciliatory', 'artful', 'thoughtless', 'deceptive', 'frugal', 'reflective', 'insulting', 'unreliable', 'stoic', 'hysterical', 'rustic', 'inhibited', 'outspoken', 'unhealthy', 'ascetic', 'skeptical', 'painstaking', 'contemplative', 'leisurely', 'sly', 'mannered', 'outrageous', 'lyrical', 'placid', 'cynical', 'irresponsible', 'vulnerable', 'arrogant', 'persuasive', 'perverse', 'steadfast', 'crisp', 'envious', 'naive', 'greedy', 'presumptuous', 'obnoxious', 'irritable', 'dishonest', 'discreet', 'sporting', 'hateful', 'ungrateful', 'frivolous', 'reactionary', 'skillful', 'cowardly', 'sordid', 'adventurous', 'dogmatic', 'intuitive', 'bland', 'indulgent', 'discontented', 'dominating', 'articulate', 'fanciful', 'discouraging', 'treacherous', 'repressed', 'moody', 'sensual', 'unfriendly', 'optimistic', 'clumsy', 'contemptible', 'focused', 'haughty', 'morbid', 'disorderly', 'considerate', 'humorous', 'preoccupied', 'airy', 'impersonal', 'cultured', 'trusting', 'respectful', 'scrupulous', 'scholarly', 'superstitious', 'tolerant', 'realistic', 'malicious', 'irrational', 'sane', 'colorless', 'masculine', 'witty', 'inert', 'prejudiced', 'fraudulent', 'blunt', 'childish', 'brittle', 'disciplined', 'responsive', 'courageous', 'bewildered', 'courteous', 'stubborn', 'aloof', 'sentimental', 'athletic', 'extravagant', 'brutal', 'manly', 'cooperative', 'unstable', 'youthful', 'timid', 'amiable', 'retiring', 'fiery', 'confidential', 'relaxed', 'imaginative', 'mystical', 'shrewd', 'conscientious', 'monstrous', 'grim', 'questioning', 'lazy', 'dynamic', 'gloomy', 'troublesome', 'abrupt', 'eloquent', 'dignified', 'hearty', 'gallant', 'benevolent', 'maternal', 'paternal', 'patriotic', 'aggressive', 'competitive', 'elegant', 'flexible', 'gracious', 'energetic', 'tough', 'contradictory', 'shy', 'careless', 'cautious', 'polished', 'sage', 'tense', 'caring', 'suspicious', 'sober', 'neat', 'transparent', 'disturbing', 'passionate', 'obedient', 'crazy', 'restrained', 'fearful', 'daring', 'prudent', 'demanding', 'impatient', 'cerebral', 'calculating', 'amusing', 'honorable', 'casual', 'sharing', 'selfish', 'ruined', 'spontaneous', 'admirable', 'conventional', 'cheerful', 'solitary', 'upright', 'stiff', 'enthusiastic', 'petty', 'dirty', 'subjective', 'heroic', 'stupid', 'modest', 'impressive', 'orderly', 'ambitious', 'protective', 'silly', 'alert', 'destructive', 'exciting', 'crude', 'ridiculous', 'subtle', 'mature', 'creative', 'coarse', 'passive', 'oppressed', 'accessible', 'charming', 'clever', 'decent', 'miserable', 'superficial', 'shallow', 'stern', 'winning', 'balanced', 'emotional', 'rigid', 'invisible', 'desperate', 'cruel', 'romantic', 'agreeable', 'hurried', 'sympathetic', 'solemn', 'systematic', 'vague', 'peaceful', 'humble', 'dull', 'expedient', 'loyal', 'decisive', 'arbitrary', 'earnest', 'confident', 'conservative', 'foolish', 'moderate', 'helpful', 'delicate', 'gentle', 'dedicated', 'hostile', 'generous', 'reliable', 'dramatic', 'precise', 'calm', 'healthy', 'attractive', 'artificial', 'progressive', 'odd', 'confused', 'rational', 'brilliant', 'intense', 'genuine', 'mistaken', 'driving', 'stable', 'objective', 'sensitive', 'neutral', 'strict', 'angry', 'profound', 'smooth', 'ignorant', 'thorough', 'logical', 'intelligent', 'extraordinary', 'experimental', 'steady', 'formal', 'faithful', 'curious', 'reserved', 'honest', 'busy', 'educated', 'liberal', 'friendly', 'efficient', 'sweet', 'surprising', 'mechanical', 'clean', 'critical', 'criminal', 'soft', 'proud', 'quiet', 'weak', 'anxious', 'solid', 'complex', 'grand', 'warm', 'slow', 'false', 'extreme', 'narrow', 'dependent', 'wise', 'organized', 'pure', 'directed', 'dry', 'obvious', 'popular', 'capable', 'secure', 'active', 'independent', 'ordinary', 'fixed', 'practical', 'serious', 'fair', 'understanding', 'constant', 'cold', 'responsible', 'deep', 'religious', 'private', 'simple', 'physical', 'original', 'working', 'strong', 'modern', 'determined', 'open', 'political', 'difficult', 'knowledge', 'kind']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racial Otherization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate distances between otherization and 'Black,' 'Asian,' and 'White' words.\n",
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c\n",
    "\n",
    "def calc_distance_between_vectors(vec1, vec2, distype = ''):\n",
    "    if distype == 'norm':\n",
    "        return np.linalg.norm(np.subtract(vec1, vec2))\n",
    "    else:\n",
    "        return cossim(vec1, vec2)\n",
    "\n",
    "# models = {}\n",
    "output = []\n",
    "\n",
    "for decade, model in [(\"1810\", model1810s),(\"1850\", model1850s),(\"1900\", model1900s),(\"1950\", model1950s), (\"2000\", model2000s)]:\n",
    "    \n",
    "    average_white_names = np.mean(np.array([model[word] for word in white_names if word in model]), axis = 0)\n",
    "    average_asian_names = np.mean(np.array([model[word] for word in asian_names if word in model]), axis = 0)\n",
    "    average_black_names = np.mean(np.array([model[word] for word in black_names if word in model]), axis = 0)\n",
    "    average_adjectives_otherization = np.mean(np.array([model[word] for word in adjectives_otherization if word in model]), axis = 0)\n",
    "#     models[decade] = [average_adjectives_otherization, average_asian_names, average_white_names,]\n",
    "   \n",
    "    row = [decade]\n",
    "    other_to_white = calc_distance_between_vectors(average_adjectives_otherization, average_white_names)\n",
    "    other_to_black = calc_distance_between_vectors(average_adjectives_otherization, average_black_names)\n",
    "    other_to_asian = calc_distance_between_vectors(average_adjectives_otherization, average_asian_names)\n",
    "    row.append(other_to_asian - other_to_white)\n",
    "    row.append(other_to_black - other_to_white)\n",
    "    output.append(row)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(pd.DataFrame(output, columns=[\"Decade\", \"Asian Otherization Bias\", \"Black Otherization Bias\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religious Otherization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate distances between otherization and 'Muslim,' and 'Christian' words.\n",
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c\n",
    "\n",
    "def calc_distance_between_vectors(vec1, vec2, distype = ''):\n",
    "    if distype == 'norm':\n",
    "        return np.linalg.norm(np.subtract(vec1, vec2))\n",
    "    else:\n",
    "        return cossim(vec1, vec2)\n",
    "\n",
    "# models = {}\n",
    "output = []\n",
    "\n",
    "for decade, model in [(\"1810\", model1810s),(\"1850\", model1850s),(\"1900\", model1900s),(\"1950\", model1950s), (\"2000\", model2000s)]:\n",
    "     \n",
    "    average_christian = np.mean(np.array([model[word] for word in words_christianity if word in model]), axis = 0)\n",
    "    average_islamic = np.mean(np.array([model[word] for word in words_islam if word in model]), axis = 0)\n",
    "    average_adjectives_otherization = np.mean(np.array([model[word] for word in adjectives_otherization if word in model]), axis = 0)\n",
    "#     models[decade] = [average_adjectives_otherization, average_asian_names, average_white_names,]\n",
    "   \n",
    "    row = [decade]\n",
    "    other_to_christian = calc_distance_between_vectors(average_adjectives_otherization, average_christian)\n",
    "    other_to_islamic = calc_distance_between_vectors(average_adjectives_otherization, average_islamic)\n",
    "    row.append(other_to_islamic - other_to_christian)\n",
    "    output.append(row)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(pd.DataFrame(output, columns=[\"Decade\", \"Asian Otherization Bias\", \"Black Otherization Bias\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personality traits - Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between occations and 'man' and 'woman' words.\n",
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c\n",
    "\n",
    "def calc_distance_between_vectors(vec1, vec2, distype = ''):\n",
    "    if distype == 'norm':\n",
    "        return np.linalg.norm(np.subtract(vec1, vec2))\n",
    "    else:\n",
    "        return cossim(vec1, vec2)\n",
    "\n",
    "decades_and_models = [(\"1810\", model1810s),(\"1850\", model1850s),(\"1900\", model1900s),(\"1950\", model1950s), (\"2000\", model2000s)]\n",
    "\n",
    "models = {}\n",
    "output = []\n",
    "\n",
    "for decade, model in decades_and_models:\n",
    "    average_white_names = np.mean(np.array([model[word] for word in white_names if word in model]), axis = 0)\n",
    "    average_asian_names = np.mean(np.array([model[word] for word in asian_names if word in model]), axis = 0)\n",
    "    average_black_names = np.mean(np.array([model[word] for word in black_names if word in model]), axis = 0)\n",
    "    models[decade] = [average_white_names, average_asian_name,average_black_names]\n",
    "\n",
    "available_traits = [p for p in personality_traits if np.any(model50s[p]) and np.any(model00s[p])]\n",
    "\n",
    "for trait in available_traits:\n",
    "    row = [trait]\n",
    "    for decade, model in decades_and_models:\n",
    "        white = calc_distance_between_vectors(model[trait], models[decade][0]) # the average vectors for men are found at index 0.\n",
    "        asian = calc_distance_between_vectors(model[trait], models[decade][1])\n",
    "        black = calc_distance_between_vectors(model[trait], models[decade][2])\n",
    "        row.append(asian - white)\n",
    "        row.append(black - white)  \n",
    "    output.append(row)\n",
    "\n",
    "\n",
    "columnnames = [decade for decade, model in decades_and_models]\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(pd.DataFrame(output, columns=columnnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some words using either or both of the models! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
